{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf21e6b",
   "metadata": {},
   "source": [
    "## SPARK - Distributed Data Processing or Cluster Computing System often used for Large Scale data processing\n",
    "### PySpark = Spark API + Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6d5076",
   "metadata": {},
   "source": [
    "# Topics:\n",
    "-  Loading Spark Session\n",
    "- Reading Dataset\n",
    "- Checking data types of columns\n",
    "- Data Preprocessing - Adding columns, Dropping columns, Renaming columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b30906",
   "metadata": {},
   "source": [
    "Installing PySpark API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeaa6156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\my files\\anaconda\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\my files\\anaconda\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727ff425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a359e8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>G</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name  Age\n",
       "0    A   21\n",
       "1    B   34\n",
       "2    C   50\n",
       "3    D   31\n",
       "4    E   25\n",
       "5    F   29\n",
       "6    G   26\n",
       "7    H   31"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('C:/Users/ASUS/OneDrive/Desktop/DatasetR/Spark_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc138e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pd.read_csv('C:/Users/ASUS/OneDrive/Desktop/DatasetR/Spark_test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3749132",
   "metadata": {},
   "source": [
    "We have to create a Spark session inorder to work with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44647fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4db0c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac729a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://CYBORG01:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x216c46f8160>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b38ca7",
   "metadata": {},
   "source": [
    "When we execute a session in a local, there will only be one cluster, but when we are working in a cloud, we can create multiple clusters and instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d20cc5",
   "metadata": {},
   "source": [
    "Reading dataset with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97f97a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading dataset using spark\n",
    "df_pyspark = spark.read.csv('C:/Users/ASUS/OneDrive/Desktop/DatasetR/Spark_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bfa5a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b4798f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "| _c0|_c1|\n",
      "+----+---+\n",
      "|Name|Age|\n",
      "|   A| 21|\n",
      "|   B| 34|\n",
      "|   C| 50|\n",
      "|   D| 31|\n",
      "|   E| 25|\n",
      "|   F| 29|\n",
      "|   G| 26|\n",
      "|   H| 31|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771009c",
   "metadata": {},
   "source": [
    "We want Name and Age as column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91a1c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('C:/Users/ASUS/OneDrive/Desktop/DatasetR/Spark_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cce8f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 21|\n",
      "|   B| 34|\n",
      "|   C| 50|\n",
      "|   D| 31|\n",
      "|   E| 25|\n",
      "|   F| 29|\n",
      "|   G| 26|\n",
      "|   H| 31|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2423aa98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92140c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='A', Age='21'), Row(Name='B', Age='34'), Row(Name='C', Age='50')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c97a9d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b4494",
   "metadata": {},
   "source": [
    "Age here is considered as a string by default which is wrong. We have to use a inferSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3903b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_1 = spark.read.option('header','true').csv('C:/Users/ASUS/OneDrive/Desktop/DatasetR/Spark_test.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e5e209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d10f9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 21|\n",
      "|   B| 34|\n",
      "|   C| 50|\n",
      "|   D| 31|\n",
      "|   E| 25|\n",
      "|   F| 29|\n",
      "|   G| 26|\n",
      "|   H| 31|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Better way\n",
    "df_pyspark_1 = spark.read.csv('C:/Users/ASUS/OneDrive/Desktop/DatasetR/Spark_test.csv',header=True,inferSchema=True)\n",
    "df_pyspark_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d1a1741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the column names\n",
    "df_pyspark_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3ac097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "|   A|\n",
      "|   B|\n",
      "|   C|\n",
      "|   D|\n",
      "|   E|\n",
      "|   F|\n",
      "|   G|\n",
      "|   H|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pick only Name column\n",
    "df_pyspark_1.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8d01f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 21|\n",
      "|   B| 34|\n",
      "|   C| 50|\n",
      "|   D| 31|\n",
      "|   E| 25|\n",
      "|   F| 29|\n",
      "|   G| 26|\n",
      "|   H| 31|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pick multiple columns\n",
    "df_pyspark_1.select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b02e1653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb701114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------------+\n",
      "|summary|Name|              Age|\n",
      "+-------+----+-----------------+\n",
      "|  count|   8|                8|\n",
      "|   mean|null|           30.875|\n",
      "| stddev|null|8.741322227541684|\n",
      "|    min|   A|               21|\n",
      "|    max|   H|               50|\n",
      "+-------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculating mean median etc\n",
    "df_pyspark_1.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "433e2e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----------------+\n",
      "|Name|Age|Age After 2 Years|\n",
      "+----+---+-----------------+\n",
      "|   A| 21|               23|\n",
      "|   B| 34|               36|\n",
      "|   C| 50|               52|\n",
      "|   D| 31|               33|\n",
      "|   E| 25|               27|\n",
      "|   F| 29|               31|\n",
      "|   G| 26|               28|\n",
      "|   H| 31|               33|\n",
      "+----+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adding new column to dataframe\n",
    "df_pyspark_2 = df_pyspark_1.withColumn('Age After 2 Years',df_pyspark_1['Age']+2)\n",
    "df_pyspark_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "568402e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 21|\n",
      "|   B| 34|\n",
      "|   C| 50|\n",
      "|   D| 31|\n",
      "|   E| 25|\n",
      "|   F| 29|\n",
      "|   G| 26|\n",
      "|   H| 31|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_3 = df_pyspark_2.drop('Age After 2 Years')\n",
    "df_pyspark_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bbe2fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|New Name|Age|\n",
      "+--------+---+\n",
      "|       A| 21|\n",
      "|       B| 34|\n",
      "|       C| 50|\n",
      "|       D| 31|\n",
      "|       E| 25|\n",
      "|       F| 29|\n",
      "|       G| 26|\n",
      "|       H| 31|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renaming Column\n",
    "df_pyspark_4 = df_pyspark_3.withColumnRenamed('Name','New Name')\n",
    "df_pyspark_4.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
