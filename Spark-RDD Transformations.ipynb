{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPbS6V3Bs+Bc37jHX+52DEZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","## Spark RDD-\n","\n","**RDD-Resilient Distributed Datasets** is a distributed collection of memory (not a distributed file system) where the data needed is always stored and kept available in RAM. It is the primary underlying data structure of Spark.\n","\n","## Features:\n","* **In-memory computation**:  In-memory computing improves the performance of data processing tasks by caching and processing data in memory, rather than reading it from disk every time it is needed. This approach significantly reduces I/O overhead and speeds up data processing, giving edge over HDFS.\n","\n","* **Lazy Evaluation**: Spark doesn't immediately execute transformations on its data; instead, it records the sequence of transformations as a logical plan and only executes them when an action is called. This deferred execution strategy offers several advantages, including optimization opportunities and fault tolerance.\n","*  **Fault tolerant through lineage and recomputation**: it ensures that data and computations remain intact and can recover from failures, such as node crashes. RDDs achieve fault tolerance through lineage and recomputation.\n"," **Lineage** - Spark maintains a lineage for each RDD. The lineage is a directed acyclic graph (DAG) that represents the sequence of transformations applied to create the RDD. Each RDD in the lineage knows how it was derived from its parent RDD(s). This lineage information is stored as metadata.\n"," **Recomputation**- In the event of a node failure or data loss, Spark can use the lineage information to recompute only the lost partitions of an RDD. Since RDDs are immutable (their content cannot be changed), Spark can reliably reapply the sequence of transformations to reconstruct the lost data.\n","* **Immutable**: Access provided by RDD is read-only. Immutability means that once you create an RDD, you cannot change its content. Instead, any operation that you perform on an RDD creates a new RDD as a result.\n","* **Partitioning**: An RDD is logically divided into smaller, manageable chunks called partitions. Partitions are the basic units of parallelism in Spark. Each partition contains a subset of the data and is processed independently by worker nodes in the cluster.\n"],"metadata":{"id":"gyQT64omNUd0"}},{"cell_type":"code","source":["!pip install pyspark\n","\n","import pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"InmNtxvbOdu3","executionInfo":{"status":"ok","timestamp":1694788186019,"user_tz":420,"elapsed":49231,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"ffaf68a1-b8a8-4b30-9e79-79ab6c2b4f81"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=1d3953d883b4b2580282be3d7db5cd4dde832f13497649d542b4c949d26857eb\n","  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.1\n"]}]},{"cell_type":"code","source":["# Creating Spark session\n","\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"APP\").getOrCreate()"],"metadata":{"id":"7ZPf4IcnQQ48","executionInfo":{"status":"ok","timestamp":1694796535936,"user_tz":420,"elapsed":1529,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["**map()**"],"metadata":{"id":"VlPDjBXdogTn"}},{"cell_type":"code","source":["numbers = [1, 2, 3, 4, 5, 6, 7]\n","numbers = spark.sparkContext.parallelize(numbers)\n","numbers.map(lambda x: x**2).collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9RuciPaLXXyi","executionInfo":{"status":"ok","timestamp":1694790419610,"user_tz":420,"elapsed":2579,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"caaa46ba-3c42-41cf-c346-fe893fe0180e"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 4, 9, 16, 25, 36, 49]"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["The parallelize function is used to create a parallelized distributed collection, typically an RDD (Resilient Distributed Dataset), from an existing collection in your driver program. It allows you to take a dataset that exists in your driver program (e.g., a list, array, or other iterable) and distribute it across the nodes of a Spark cluster for parallel processing."],"metadata":{"id":"Af7U2lsvghxu"}},{"cell_type":"code","source":["lists = ['alpha', 'sigma', 'beta']\n","lists = spark.sparkContext.parallelize(lists)\n","lists.map(lambda line: line.upper()).collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmDoHVWYgk_b","executionInfo":{"status":"ok","timestamp":1694794589276,"user_tz":420,"elapsed":680,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"50a7d2c8-40d9-49ea-f1c9-635284026ea5"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['ALPHA', 'SIGMA', 'BETA']"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["**flatMap()** is similar to map, but it can return an iterable of multiple elements for each input element. It flattens the results into a single iterable."],"metadata":{"id":"bdbydI0lonXG"}},{"cell_type":"code","source":["\n","original_list = [1, 2, 3, 4, 5]\n","mapped_list = [item for sublist in map(lambda x: [x, x * 2], original_list) for item in sublist]\n","\n","print(mapped_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"reX8qPzQoovD","executionInfo":{"status":"ok","timestamp":1694796494698,"user_tz":420,"elapsed":144,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"22bf03fb-ed30-4d14-ba2d-21cc64c27bca"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 2, 4, 3, 6, 4, 8, 5, 10]\n"]}]},{"cell_type":"markdown","source":["The flatMap function is a feature of Apache Spark's RDD (Resilient Distributed Dataset) and other similar functional programming libraries, not a built-in Python list method."],"metadata":{"id":"8GYgg1ZdwSYc"}},{"cell_type":"code","source":["\n","original_list = [1, 2, 3, 4, 5]\n","\n","original_list = spark.sparkContext.parallelize(original_list)\n","\n","# Using flatMap to create a new list with each element and its double\n","result_list = original_list.flatMap(lambda x: [x, x * 2]).collect()\n","\n","# Display the result\n","print(result_list)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hbo7uvBQvkAQ","executionInfo":{"status":"ok","timestamp":1694796620434,"user_tz":420,"elapsed":694,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"7ffe218a-3160-448d-f989-7b3c425277b2"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 2, 4, 3, 6, 4, 8, 5, 10]\n"]}]},{"cell_type":"markdown","source":["Transforms an RDD so that each RDD element can be converted to multiple\n","new elements with the provided function"],"metadata":{"id":"K9O4fY56xI48"}},{"cell_type":"code","source":["texts = [\"Quick, Fox\", \"Lazy, Dog\"]\n","texts = spark.sparkContext.parallelize(texts)\n","texts.flatMap(lambda line: line.split(', ')).collect()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfB1nXQ9xSNa","executionInfo":{"status":"ok","timestamp":1694797065273,"user_tz":420,"elapsed":886,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"51add072-ace7-4e8a-e6e0-77f8870be510"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Quick', 'Fox', 'Lazy', 'Dog']"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## filter()\n","can be used to trim out information that you don’t need"],"metadata":{"id":"5rj_Cv78Cy_N"}},{"cell_type":"code","source":["texts = [\"Instructor, RasAlGhul\", \"Student 1, Batman\", \"Student 2, Arrow\"]\n","texts = spark.sparkContext.parallelize(texts)\n","texts.filter(lambda line: \"Student\" in line).collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LUf0e5Jzx5Qy","executionInfo":{"status":"ok","timestamp":1694801412972,"user_tz":420,"elapsed":1334,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"c190ce45-0c20-49c0-f9fc-e1f3b0527303"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Student 1, Batman', 'Student 2, Arrow']"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["#Without RDD code\n","\n","texts = [\"Instructor, RasAlGhul\", \"Student 1, Batman\", \"Student 2, Arrow\"]\n","list(filter(lambda line: \"Student\" in line, texts))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lp8IzwlVCVy6","executionInfo":{"status":"ok","timestamp":1694801469679,"user_tz":420,"elapsed":151,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"21f51806-335b-4aec-b405-69e34c9f798c"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Student 1, Batman', 'Student 2, Arrow']"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["## distinct()\n","return a new RDD that contains distinct elements of the source RDD"],"metadata":{"id":"f22GXxaOFeFr"}},{"cell_type":"code","source":["\n","data = spark.sparkContext.parallelize([1, 1, 3, -5, 'data', 'data'])\n","print(data.distinct().collect())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1m6wEodFhaB","executionInfo":{"status":"ok","timestamp":1694802404575,"user_tz":420,"elapsed":1982,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"53898727-8549-4b8f-c72c-eacc41d395c6"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 3, -5, 'data']\n"]}]},{"cell_type":"markdown","source":["## groupByKey()\n","\n","groupByKey() transformation is used to group the elements of an RDD (Resilient Distributed Dataset) by a key or a key-value pair. It groups together all the elements that have the same key into a sequence (an iterable) of values associated with that key"],"metadata":{"id":"E-Q57XZBHUsD"}},{"cell_type":"code","source":["\n","# Create an RDD with key-value pairs\n","data = [(1, 'A'), (2, 'B'), (1, 'C'), (2, 'D'), (3, 'E')]\n","rdd = spark.sparkContext.parallelize(data)\n","\n","# Use groupByKey() to group the data by keys\n","grouped_rdd = rdd.groupByKey()\n","\n","# Iterate through the grouped data\n","for key, values in grouped_rdd.collect():\n","    print(f\"Key: {key}, Values: {list(values)}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fg-pLucRHbqF","executionInfo":{"status":"ok","timestamp":1694802941966,"user_tz":420,"elapsed":882,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"8e6eceb6-92a1-48ce-b86a-87780c02854c"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Key: 2, Values: ['B', 'D']\n","Key: 1, Values: ['A', 'C']\n","Key: 3, Values: ['E']\n"]}]},{"cell_type":"markdown","source":["While groupByKey() is a useful transformation, it may not be the most efficient option for all use cases, especially when dealing with large datasets. In some cases, you might prefer to use operations like reduceByKey(), aggregateByKey(), or combineByKey() for grouped operations, as they provide better performance optimizations by reducing data shuffling."],"metadata":{"id":"XMhrrvFbHQQ4"}},{"cell_type":"markdown","source":["## reduceByKey()\n","\n","In Apache Spark, the reduceByKey() transformation is used to perform a reduction operation on the values associated with each key in an RDD (Resilient Distributed Dataset). It combines values with the same key using a specified function and produces a new RDD with one entry for each distinct key."],"metadata":{"id":"g2UahXcAWKvR"}},{"cell_type":"code","source":["data = [(\"A\", 2), (\"B\", 3), (\"A\", 4), (\"B\", 5), (\"C\", 1)]\n","rdd = spark.sparkContext.parallelize(data)\n","\n","# Use reduceByKey() to sum the values for each key\n","sums = rdd.reduceByKey(lambda x, y: x + y)\n","\n","# Collect and display the results\n","result = sums.collect()\n","for key, value in result:\n","    print(f\"Key: {key}, Sum: {value}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j4UA3JhqWJ_L","executionInfo":{"status":"ok","timestamp":1694806930827,"user_tz":420,"elapsed":1083,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"31736bf8-a19a-4724-cbd6-52b834897816"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Key: C, Sum: 1\n","Key: A, Sum: 6\n","Key: B, Sum: 8\n"]}]},{"cell_type":"markdown","source":["reduceByKey() is a powerful transformation for aggregating data based on keys and can be used for a variety of operations beyond just summing values, such as finding the maximum, minimum, or performing custom aggregations. It's particularly efficient because it performs the aggregation locally on each partition before shuffling the data across partitions, which reduces data movement and improves performance."],"metadata":{"id":"4gGUmtP2YLbf"}},{"cell_type":"markdown","source":["## sortBy()\n","\n","In Apache Spark, the sortBy() transformation is used to sort the elements of an RDD (Resilient Distributed Dataset) or DataFrame based on a specified ordering criterion. It allows you to control the sorting order (ascending or descending) and specify the key by which you want to sort the data."],"metadata":{"id":"6XwSJfRuYM_K"}},{"cell_type":"code","source":["\n","data = [5, 2, 8, 1, 7]\n","rdd = spark.sparkContext.parallelize(data)\n","\n","# Use sortBy() to sort the RDD in ascending order\n","sorted_rdd = rdd.sortBy(lambda x: x)\n","\n","# Collect and display the sorted result\n","sorted_data = sorted_rdd.collect()\n","print(sorted_data)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4UQO88hYukK","executionInfo":{"status":"ok","timestamp":1694807352230,"user_tz":420,"elapsed":1954,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"63ff2e88-35cb-44fc-96c9-be985e8a18db"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 5, 7, 8]\n"]}]},{"cell_type":"code","source":["data = spark.sparkContext.parallelize([('Z', 99), ('B', 3), ('C', 4)])\n","## Sort RDD by values in the ascending order\n","print(data.sortBy(lambda pair: pair[1]).collect())\n","## Sort RDD by values in the descending order\n","print(data.sortBy(lambda pair: -pair[1]).collect())\n","## Sort RDD by keys\n","print(data.sortBy(lambda pair: pair[0]).collect())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXq1niahauIk","executionInfo":{"status":"ok","timestamp":1694807845453,"user_tz":420,"elapsed":5246,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"8fd0cb18-915b-48de-c1b1-114cfd70e4e1"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["[('B', 3), ('C', 4), ('Z', 99)]\n","[('Z', 99), ('C', 4), ('B', 3)]\n","[('B', 3), ('C', 4), ('Z', 99)]\n"]}]},{"cell_type":"code","source":["data1 = spark.sparkContext.parallelize(['Arizona', 'California', 'Texas'])\n","data2 = spark.sparkContext.parallelize(['Arizona', 'Nevada'])"],"metadata":{"id":"kOi4kaOWbaTs","executionInfo":{"status":"ok","timestamp":1694808042828,"user_tz":420,"elapsed":133,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["## subtract()"],"metadata":{"id":"W-igxSFBcfhF"}},{"cell_type":"markdown","source":["**subtract()** transformation is used to compute the set difference between two RDDs (Resilient Distributed Datasets). It returns an RDD containing only the elements that exist in the first RDD but not in the second RDD, effectively subtracting the elements of the second RDD from the first RDD."],"metadata":{"id":"YEVBWHzAb7Hd"}},{"cell_type":"code","source":["print(data1.subtract(data2).collect())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztG8bjnmbnOF","executionInfo":{"status":"ok","timestamp":1694808052274,"user_tz":420,"elapsed":2170,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"3e29c627-cfe2-4f1f-fd5d-8308352b4dcd"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["['Texas', 'California']\n"]}]},{"cell_type":"markdown","source":["## intersection()"],"metadata":{"id":"_AhJ2_Focjez"}},{"cell_type":"markdown","source":["**intersection()** returns a new RDD containing elements that are common to both of the input RDDs, effectively finding the set intersection of the two RDDs."],"metadata":{"id":"gKZNifP5b_QY"}},{"cell_type":"code","source":["print(data1.intersection(data2).collect())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KI2bUgp2brD_","executionInfo":{"status":"ok","timestamp":1694808080600,"user_tz":420,"elapsed":2845,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"093a7cb9-5896-4a35-ac61-df9a4f06180c"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["['Arizona']\n"]}]},{"cell_type":"markdown","source":["## union()"],"metadata":{"id":"eiwW-Dz8cmz3"}},{"cell_type":"markdown","source":["**union()** transformation is used to combine two RDDs (Resilient Distributed Datasets) into a single RDD. It creates a new RDD that contains all the elements from both input RDDs without removing any duplicates."],"metadata":{"id":"-9t8gwHYcPmJ"}},{"cell_type":"code","source":["print(data1.union(data2).collect())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lm9hDqfnbwy2","executionInfo":{"status":"ok","timestamp":1694808100037,"user_tz":420,"elapsed":136,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"f1026e13-a255-4580-af3c-1620382b1094"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["['Arizona', 'California', 'Texas', 'Arizona', 'Nevada']\n"]}]},{"cell_type":"markdown","source":["## Spark Actions"],"metadata":{"id":"peAkOGX7dAeU"}},{"cell_type":"markdown","source":["**collect()** action retrieves all the elements of an RDD or DataFrame and returns them to the driver program as an array or list. Be cautious when using collect() with large datasets because it can potentially exhaust the driver's memory."],"metadata":{"id":"aD2-XRFhdC3h"}},{"cell_type":"code","source":["data = spark.sparkContext.parallelize([1, 3, 1, 3, 3, -2])\n","print(data.collect())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LHr8HqG8fA72","executionInfo":{"status":"ok","timestamp":1694809021560,"user_tz":420,"elapsed":158,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"4975398a-1f76-4591-8149-eb8bee5ffc7c"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 3, 1, 3, 3, -2]\n"]}]},{"cell_type":"markdown","source":["**count()** action returns the number of elements in an RDD or the number of rows in a DataFrame."],"metadata":{"id":"zv5CL3ksfZds"}},{"cell_type":"code","source":["data = spark.sparkContext.parallelize([1, 3, 1, 3, 3, -2])\n","print(data.count())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KPOTaHUwfiz6","executionInfo":{"status":"ok","timestamp":1694809127122,"user_tz":420,"elapsed":669,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"3da7940b-6084-424a-b4f0-156ee01ab236"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["6\n"]}]},{"cell_type":"markdown","source":["**take(n)** action returns the first n elements of an RDD or the first n rows of a DataFrame as an array or list."],"metadata":{"id":"hitDeFwyf_oN"}},{"cell_type":"code","source":["data = spark.sparkContext.parallelize([1, 3, 1, 3, 3, -2])\n","print(data.take(2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4or3zcU1gC1B","executionInfo":{"status":"ok","timestamp":1694809243786,"user_tz":420,"elapsed":326,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"09816c70-bc2d-428e-b621-8dd6ad929ae6"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 3]\n"]}]},{"cell_type":"markdown","source":["**countByValue()** return how many times each element occur in the RDD"],"metadata":{"id":"3cbJ5VSbgODm"}},{"cell_type":"code","source":["data = spark.sparkContext.parallelize([\"A\", \"B\", \"A\", \"A\", \"C\", \"B\"])\n","print(data.countByValue())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wi7T_G1ngfC0","executionInfo":{"status":"ok","timestamp":1694809519276,"user_tz":420,"elapsed":748,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"08ca5a6a-afac-4195-f960-9860c81893e4"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["defaultdict(<class 'int'>, {'A': 3, 'B': 2, 'C': 1})\n"]}]},{"cell_type":"markdown","source":["**reduce(func)** action aggregates the elements of an RDD using a specified function. It's commonly used for computing sums, products, or custom aggregations."],"metadata":{"id":"dcrSY5iFhYug"}},{"cell_type":"code","source":["data = [1, 2, 3, 4, 5]\n","rdd = spark.sparkContext.parallelize(data)\n","\n","# Use the reduce() action to compute the sum of elements\n","def add(x, y):\n","    return x + y\n","\n","sum_result = rdd.reduce(add)\n","\n","# Display the sum result\n","print(\"Sum of elements:\", sum_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qira692uhi5S","executionInfo":{"status":"ok","timestamp":1694809695107,"user_tz":420,"elapsed":734,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}},"outputId":"044e07b4-df8c-471c-dcb8-41df321884f3"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Sum of elements: 15\n"]}]},{"cell_type":"markdown","source":["**saveAsTextFile(path)** save the RDD to files in avdirectory. Will create the directory if it doesn’t exist and will fail if it does."],"metadata":{"id":"APUK5Bnjoe2r"}},{"cell_type":"code","source":["data = [\"Hello\", \"World\", \"Apache\", \"Spark\"]\n","rdd = spark.sparkContext.parallelize(data)\n","\n","# Specify the directory where you want to save the text files\n","output_dir = \"/content/drive/MyDrive/output.txt\"\n","\n","# Use saveAsTextFile() to save the RDD elements as text files\n","rdd.saveAsTextFile(output_dir)"],"metadata":{"id":"vpjJ-iKgkPZb","executionInfo":{"status":"ok","timestamp":1694811268391,"user_tz":420,"elapsed":767,"user":{"displayName":"Sarthak Haldar (EndeavouR)","userId":"04534801659356034583"}}},"execution_count":51,"outputs":[]}]}